{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f14dbfd3-f967-439c-b158-7e19b9bafbc4",
   "metadata": {},
   "source": [
    "## Sensitivity Analysis\n",
    "- DAS Article Suggestions\n",
    "- 25-Dec-2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7ee21e98-27ae-48ee-8332-e45cbcbbb42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " == REINFORCE for Predictive Maintenance ==\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "* Experiments file: SensitivityAnalysis.csv -- Rounds 1\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "START_ROUND = 0\n",
    "TRAINING_ROUNDS = 1\n",
    "EXPTS_SETTINGS = 'SensitivityAnalysis.csv'\n",
    "# EXPTS_SETTINGS = 'SA_test.csv'\n",
    "MIN_MODEL_PERFORMANCE = -1.0 # Set to 0.70, to auto save models with metrics > 0.7\n",
    "\n",
    "print ('\\n == REINFORCE for Predictive Maintenance ==')\n",
    "print (120*'-')\n",
    "print (f'* Experiments file: {EXPTS_SETTINGS} -- Rounds {TRAINING_ROUNDS}')\n",
    "print (120*'=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3067e0ed-9843-47eb-8b71-215ff661b471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy network learning parameters\n",
    "gamma = 0.99\n",
    "\n",
    "learning_rates = [1e-2, 0.5e-2, 1e-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "23994053-ed36-4ad9-b558-99c78d41bbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Loading packages...\n"
     ]
    }
   ],
   "source": [
    "print ('- Loading packages...')\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "from milling_tool_environment import MillingTool_SS_NT, MillingTool_MS_V3\n",
    "from utilities import compute_metrics, compute_metrics_simple, write_metrics_report, store_results, plot_learning_curve, single_axes_plot, lnoise\n",
    "from utilities import two_axes_plot, two_variable_plot, plot_error_bounds, test_script, write_test_results, downsample, save_model, load_model, clean_up_files\n",
    "from utilities import add_performance_columns, summary_performance_metrics, sensitivity_anlysis_metrics\n",
    "from reinforce_classes import PolicyNetwork, Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cb8cee1a-4368-4a74-88d4-129d87f245eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_summary = []\n",
    "n_tr_round = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "21fc0c57-a7a6-41fd-b4cd-a0a09dfab340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Loading Experiments...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 3)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Auto experiment file structure\n",
    "print ('- Loading Experiments...')\n",
    "df_expts = pd.read_csv(EXPTS_SETTINGS)\n",
    "\n",
    "# Add round number column to Experiments files\n",
    "df_expts['Round'] = n_tr_round\n",
    "# Initialize columns for recording model performance\n",
    "df_expts = add_performance_columns(df_expts)\n",
    "df_expts['model_file'] = 'Not satisfactory'\n",
    "\n",
    "# Initialize record training time\n",
    "df_expts['RF_time'] = 0.0\n",
    "df_expts['A2C_time'] = 0.0\n",
    "df_expts['DQN_time'] = 0.0\n",
    "df_expts['PPO_time'] = 0.0\n",
    "\n",
    "n_expts = len(df_expts.index)\n",
    "n_expt = 0\n",
    "n_expt, n_expts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1c50949f-0c79-465d-b352-b03155e13412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " **** Sensitivity Analysis: LR: 0.01 ****\n",
      "\n",
      "\n",
      "\n",
      "************************************************************************************************************************\n",
      "************************************************************************************************************************\n",
      " * Round-0 Experiment 0 | PHM_C01_SS_NoNBD\n",
      "========================================================================================================================\n",
      "\n",
      "- Columns added to results file:  results//0_0_PHM_C01_SS_NoNBD__test_results_25-Dec-2024_13-32.csv\n"
     ]
    }
   ],
   "source": [
    "dt = datetime.now()\n",
    "dt_d = dt.strftime('%d-%b-%Y')\n",
    "dt_t = dt.strftime('%H-%M')\n",
    "dt_m = f'{dt_d}_{dt_t}'\n",
    "# dt_m = dt.strftime('%d-%H%M')\n",
    "\n",
    "# Load experiment parameters\n",
    "ENVIRONMENT_CLASS = df_expts['environment'][n_expt]\n",
    "ENVIRONMENT_INFO = df_expts['environment_info'][n_expt]\n",
    "ENVIRONMENT_INFO = f'{ENVIRONMENT_INFO}-{ENVIRONMENT_CLASS}'\n",
    "DATA_FILE = df_expts['data_file'][n_expt]\n",
    "R1 = df_expts['R1'][n_expt]\n",
    "R2 = df_expts['R2'][n_expt]\n",
    "R3 = df_expts['R3'][n_expt]\n",
    "WEAR_THRESHOLD = df_expts['wear_threshold'][n_expt]\n",
    "THRESHOLD_FACTOR = df_expts['threshold_factor'][n_expt]\n",
    "ADD_NOISE = df_expts['add_noise'][n_expt]\n",
    "BREAKDOWN_CHANCE = df_expts['breakdown_chance'][n_expt]\n",
    "EPISODES = df_expts['episodes'][n_expt]\n",
    "MILLING_OPERATIONS_MAX = df_expts['milling_operations_max'][n_expt]\n",
    "ver_prefix = df_expts['version_prefix'][n_expt]\n",
    "TEST_INFO = df_expts['test_info'][n_expt]\n",
    "TEST_CASES = df_expts['test_cases'][n_expt]\n",
    "TEST_ROUNDS = df_expts['test_rounds'][n_expt]\n",
    "RESULTS_FOLDER = df_expts['results_folder'][n_expt]\n",
    "\n",
    "TEST_FILE = df_expts['test_file'][n_expt]\n",
    "TRAIN_SR = df_expts['train_sample_rate'][n_expt]\n",
    "TEST_SR = df_expts['test_sample_rate'][n_expt]\n",
    "\n",
    "alpha = df_expts['LR'][n_expt]  # Learning rate\n",
    "print (f'\\n **** Sensitivity Analysis: LR: {alpha} ****')\n",
    "\n",
    "## Read data\n",
    "df = pd.read_csv(DATA_FILE)\n",
    "n_records = len(df.index)\n",
    "l_noise = lnoise(ADD_NOISE, BREAKDOWN_CHANCE)\n",
    "#VERSION = f'{ver_prefix}_{l_noise}_{WEAR_THRESHOLD}_{THRESHOLD_FACTOR}_{R3}_{EPISODES}_{MILLING_OPERATIONS_MAX}_'\n",
    "VERSION = f'{n_tr_round}_{n_expt}_{ver_prefix}_{l_noise}_'\n",
    "print('\\n\\n')\n",
    "print(120*'*')\n",
    "print(120*'*')\n",
    "print(f' * Round-{n_tr_round} Experiment {n_expt} | {ver_prefix}_{l_noise}')\n",
    "print(120*'=')\n",
    "\n",
    "METRICS_METHOD = 'binary' # average method = {‘micro’, ‘macro’, ‘samples’, ‘weighted’, ‘binary’}\n",
    "WEAR_THRESHOLD_NORMALIZED = 0.0 # normalized to the max wear threshold\n",
    "\n",
    "CONSOLIDATED_METRICS_FILE = f'{RESULTS_FOLDER}/TEST_CONSOLIDATED_METRICS.csv'\n",
    "RESULTS_FILE = f'{RESULTS_FOLDER}/{VERSION}_test_results_{dt_m}.csv'\n",
    "METRICS_FILE = f'{RESULTS_FOLDER}/{VERSION}_metrics.csv'\n",
    "RF_TRAINING_METRICS = f'{RESULTS_FOLDER}/{VERSION}_Sensitivity_Analysis_LR_{alpha}.csv'\n",
    "\n",
    "END_ROUND = START_ROUND + TRAINING_ROUNDS\n",
    "EXPTS_REPORT = f'{RESULTS_FOLDER}/Experiment_Results_{START_ROUND}_{END_ROUND}_{n_tr_round}.csv'\n",
    "\n",
    "logdir = './tensorboard/'\n",
    "\n",
    "print('\\n- Columns added to results file: ', RESULTS_FILE)\n",
    "results = ['Date', 'Time', 'Round', 'Environment', 'Training_data', 'Wear_Threshold', 'Test_data', 'Algorithm', 'Episodes', 'Normal_cases', 'Normal_error',\n",
    "           'Replace_cases', 'Replace_error', 'Overall_error',\n",
    "           'Precision', 'Recall', 'F_Beta_0_5', 'F_Beta_0_75', 'F_1_Score']\n",
    "write_test_results(results, RESULTS_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "79689c7d-e60a-4108-8c8b-89a550bb87a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Tool wear data imported (34722 records).\n"
     ]
    }
   ],
   "source": [
    "# 1. Add noise\n",
    "if ADD_NOISE:\n",
    "    df['tool_wear'] += np.random.normal(0, 1, n_records)/ADD_NOISE\n",
    "\n",
    "# 2. Add ACTION code\n",
    "df['ACTION_CODE'] = np.where(df['tool_wear'] < WEAR_THRESHOLD, 0.0, 1.0)\n",
    "\n",
    "# 3. Normalize\n",
    "WEAR_MIN = df['tool_wear'].min()\n",
    "WEAR_MAX = df['tool_wear'].max()\n",
    "WEAR_THRESHOLD_ORG_NORMALIZED = (WEAR_THRESHOLD-WEAR_MIN)/(WEAR_MAX-WEAR_MIN)\n",
    "WEAR_THRESHOLD_NORMALIZED = THRESHOLD_FACTOR*(WEAR_THRESHOLD-WEAR_MIN)/(WEAR_MAX-WEAR_MIN)\n",
    "df_normalized = (df-df.min())/(df.max()-df.min())\n",
    "df_normalized['ACTION_CODE'] = df['ACTION_CODE']\n",
    "print(f'- Tool wear data imported ({len(df.index)} records).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "622846a6-9278-4074-b7f2-29f7bc831d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Down-sampling. Input data records: 34722. Sampling rate: 100. Expected rows 347. Down-sampled to 348 rows.\n",
      "- Down-sampling. Input data records: 34722. Sampling rate: 70. Expected rows 496. Down-sampled to 497 rows.\n",
      "- Tool wear data split into train (348 records) and test (497 records).\n"
     ]
    }
   ],
   "source": [
    "# 4. Test file -or- create test file\n",
    "if TRAIN_SR:\n",
    "    # 4. Split into train and test\n",
    "    df_train = downsample(df_normalized, TRAIN_SR)\n",
    "    df_train.to_csv('TempTrain.csv')\n",
    "    df_train = pd.read_csv('TempTrain.csv')\n",
    "\n",
    "    df_test = downsample(df_normalized, TEST_SR)\n",
    "    df_test.to_csv('TempTest.csv')\n",
    "    df_test = pd.read_csv('TempTest.csv')\n",
    "    print(f'- Tool wear data split into train ({len(df_train.index)} records) and test ({len(df_test.index)} records).')\n",
    "else:\n",
    "    # 4. Split into train and test\n",
    "    df_train = df_normalized\n",
    "    df_test = pd.read_csv(TEST_FILE)\n",
    "    print(f'* Separate test data provided: {TEST_FILE} - ({len(df_test.index)} records).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "910212c4-95e0-48ce-93f6-2d3b5adee9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_records = len(df_train.index)\n",
    "x = [n for n in range(n_records)]\n",
    "y1 = df_train['tool_wear']\n",
    "y2 = df_train['ACTION_CODE']\n",
    "wear_plot = f'{RESULTS_FOLDER}/{VERSION}_wear_plot.png'\n",
    "title=f'Tool Wear (mm) data\\n{VERSION}'\n",
    "two_axes_plot(x, y1, y2, title=title, x_label='Time', y1_label='Tool Wear (mm)', y2_label='Action code (1=Replace)', xticks=20, file=wear_plot, threshold_org = WEAR_THRESHOLD_ORG_NORMALIZED, threshold=WEAR_THRESHOLD_NORMALIZED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1606c87d-f512-437e-b635-1e8f4f54f5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAS RUN\n",
      "\n",
      "\n",
      "** -- Single-variate env. wear_threshold: 0.5170 R1: 1, R2: -1, R3: -40. Noise: 0. Break-down chance: 0 -- **\n",
      "DAS RUN\n",
      "\n",
      "\n",
      "** -- Single-variate env. wear_threshold: 0.5744 R1: 1, R2: -1, R3: -40. Noise: 0. Break-down chance: 0 -- **\n"
     ]
    }
   ],
   "source": [
    "if ENVIRONMENT_CLASS == 'SS':\n",
    "    env = MillingTool_SS_NT(df_train, WEAR_THRESHOLD_NORMALIZED, MILLING_OPERATIONS_MAX, ADD_NOISE, BREAKDOWN_CHANCE, R1, R2, R3)\n",
    "    env_test = MillingTool_SS_NT(df_test, WEAR_THRESHOLD_ORG_NORMALIZED, MILLING_OPERATIONS_MAX, ADD_NOISE, BREAKDOWN_CHANCE, R1, R2, R3)\n",
    "elif ENVIRONMENT_CLASS == 'MS':\n",
    "    env = MillingTool_MS_V3(df_train, WEAR_THRESHOLD_NORMALIZED, MILLING_OPERATIONS_MAX, ADD_NOISE, BREAKDOWN_CHANCE, R1, R2, R3)\n",
    "    env_test = MillingTool_MS_V3(df_test, WEAR_THRESHOLD_ORG_NORMALIZED, MILLING_OPERATIONS_MAX, ADD_NOISE, BREAKDOWN_CHANCE, R1, R2, R3)\n",
    "else:\n",
    "    print(' ERROR - initatizing environment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "447931e7-33fb-47ae-8c0e-0a92413a9bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* Train REINFORCE model...\n"
     ]
    }
   ],
   "source": [
    "# Enable tensorboard rewards and loss plots\n",
    "# env = Monitor(env, logdir, allow_early_resets=True)\n",
    "# print('----NO MONITOR----')\n",
    "\n",
    "# ## REINFORCE RL Algorithm\n",
    "### Main loop\n",
    "print('\\n* Train REINFORCE model...')\n",
    "rewards_history = []\n",
    "loss_history = []\n",
    "training_stats = []\n",
    "\n",
    "input_dim = env.observation_space.shape[0]\n",
    "output_dim = env.action_space.n\n",
    "\n",
    "agent_RF = Agent(input_dim, output_dim, alpha, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4339df8c-1586-417c-94b2-6361195628b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0000] Loss:  -7.83e+01 | Reward:  -1.34e+00 | Ep.length: 0348\n",
      "[0100] Loss:   8.01e-01 | Reward:   1.85e+00 | Ep.length: 0348\n",
      "[0200] Loss:  -8.83e-01 | Reward:   1.69e+00 | Ep.length: 0348\n",
      "[0300] Loss:   4.51e+00 | Reward:   3.52e+00 | Ep.length: 0348\n",
      "[0400] Loss:   5.99e+00 | Reward:   4.44e+00 | Ep.length: 0348\n",
      "[0500] Loss:   1.02e+01 | Reward:   3.44e+00 | Ep.length: 0348\n",
      "[0600] Loss:   6.00e+00 | Reward:   4.04e+00 | Ep.length: 0348\n",
      "[0700] Loss:   6.07e+00 | Reward:   4.48e+00 | Ep.length: 0348\n"
     ]
    }
   ],
   "source": [
    "# $$$ \n",
    "truncated = False\n",
    "\n",
    "time_RF = time.time()\n",
    "for episode in range(EPISODES):\n",
    "    state = env.reset()\n",
    "\n",
    "    # Sample a trajectory\n",
    "    for t in range(MILLING_OPERATIONS_MAX): # Max. milling operations desired\n",
    "        action = agent_RF.act(state)\n",
    "        # state, reward, done, truncated, info = env.step(action)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        agent_RF.rewards.append(reward)\n",
    "        #env.render()\n",
    "        if done:\n",
    "            # print('** DONE **', info)\n",
    "            break\n",
    "\n",
    "    # Learn during this episode\n",
    "    loss = agent_RF.learn() # train per episode\n",
    "    total_reward = sum(agent_RF.rewards)\n",
    "\n",
    "    # Record statistics for this episode\n",
    "    rewards_history.append(total_reward)\n",
    "    loss_history.append(loss.item()) # Extract values from list of torch items for plotting\n",
    "\n",
    "    # On-policy - so discard all data\n",
    "    agent_RF.onpolicy_reset()\n",
    "\n",
    "    if (episode%100 == 0):\n",
    "        # print(f'[{episode:04d}] Loss: {loss:>10.2f} | Reward: {total_reward:>10.2f} | Ep.length: {env.ep_length:04d}')\n",
    "        print(f'[{episode:04d}] Loss: {loss:>10.2e} | Reward: {total_reward:>10.2e} | Ep.length: {env.ep_length:04d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e6b947af-5f07-49d3-90da-6a12d1961e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# end RF training\n",
    "time_RF = time.time() - time_RF\n",
    "\n",
    "x = [i for i in range(EPISODES)]\n",
    "\n",
    "# ## Moving average for rewards\n",
    "ma_window_size = 10\n",
    "# # Convert error array to pandas series\n",
    "rewards = pd.Series(rewards_history)\n",
    "windows = rewards.rolling(ma_window_size)\n",
    "moving_avg = windows.mean()\n",
    "moving_avg_lst = moving_avg.tolist()\n",
    "y1 = rewards\n",
    "y2 = moving_avg_lst\n",
    "\n",
    "filename = f'{RESULTS_FOLDER}/{VERSION}_Avg_episode_rewards.png'\n",
    "two_variable_plot(x, y1, y2, 'Avg. rewards per episode', VERSION, 'Episode', 'Avg. Rewards', 'Moving Avg.', 50, filename)\n",
    "\n",
    "# plot_error_bounds(x, y1)\n",
    "\n",
    "filename = f'{RESULTS_FOLDER}/{VERSION}_Episode_Length.png'\n",
    "single_axes_plot(x, env.ep_length_history, 'Episode length', VERSION, 'Episode', 'No of milling operations', 50, 0.0, filename)\n",
    "\n",
    "filename = f'{RESULTS_FOLDER}/{VERSION}_Tool_Replacements.png'\n",
    "single_axes_plot(x, env.ep_tool_replaced_history, 'Tool replacements per episode', VERSION, 'Episode', 'Replacements', 50, 0.0, filename)\n",
    "\n",
    "# ### Generate a balanced test set\n",
    "idx_replace_cases = df_test.index[df_test['ACTION_CODE'] >= 1.0]\n",
    "idx_normal_cases = df_test.index[df_test['ACTION_CODE'] < 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a41b9519-b4de-462f-ad14-2f9878204a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- Store SENSITIVITY analysis results i.e. REINFORCE training metrics...\n",
      "REINFORCE algorithm results saved to results//0_0_PHM_C01_SS_NoNBD__Sensitivity_Analysis_LR_0.01.csv\n",
      "\n",
      "- Test REINFORCE model...\n",
      "*\n",
      "========================================================================================================================\n",
      "[0:0] >> PHM C01 SS NBD-SS - NoNBD Pr: 1.000 \t Rc: 0.665 \t F1:0.907\n",
      "========================================================================================================================\n",
      "*\n",
      "- REINFORCE Test results written to file: results//0_0_PHM_C01_SS_NoNBD__test_results_25-Dec-2024_13-32.csv.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Process results\n",
    "training_round = 0\n",
    "print('\\n- Store SENSITIVITY analysis results i.e. REINFORCE training metrics...')\n",
    "eps = [i for i in range(EPISODES)]\n",
    "store_results(RF_TRAINING_METRICS, training_round, eps, rewards_history, env.ep_tool_replaced_history)\n",
    "\n",
    "print('\\n- Test REINFORCE model...')\n",
    "avg_Pr = avg_Rc = avg_F1 = 0.0\n",
    "\n",
    "test_case_collection = []\n",
    "for test_round in range(TEST_ROUNDS):\n",
    "    # Create test cases\n",
    "    idx_replace_cases = np.random.choice(idx_replace_cases, int(TEST_CASES/2), replace=False)\n",
    "    idx_normal_cases = np.random.choice(idx_normal_cases, int(TEST_CASES/2), replace=False)\n",
    "    test_cases = [*idx_normal_cases, *idx_replace_cases]\n",
    "    test_case_collection.append(test_cases)\n",
    "\n",
    "    results = test_script(METRICS_METHOD, test_round, df_test, 'REINFORCE', EPISODES, env_test, ENVIRONMENT_INFO, agent_RF,\n",
    "                          test_cases, TEST_INFO, DATA_FILE, WEAR_THRESHOLD, RESULTS_FILE)\n",
    "    write_test_results(results, RESULTS_FILE)\n",
    "    avg_Pr += results[14]\n",
    "    avg_Rc += results[15]\n",
    "    avg_F1 += results[16]\n",
    "\n",
    "avg_Pr = avg_Pr/TEST_ROUNDS\n",
    "avg_Rc = avg_Rc/TEST_ROUNDS\n",
    "avg_F1 = avg_F1/TEST_ROUNDS\n",
    "\n",
    "# df_expts.loc[n_expt, 'Pr'] = avg_Pr\n",
    "# df_expts.loc[n_expt, 'Rc'] = avg_Rc\n",
    "# df_expts.loc[n_expt, 'F1'] = avg_F1\n",
    "\n",
    "df_expts.loc[n_expt, 'RF_time'] = time_RF\n",
    "\n",
    "expt_summary = f'[{n_tr_round}:{n_expt}] >> {ENVIRONMENT_INFO} - {l_noise} Pr: {avg_Pr:0.3f} \\t Rc: {avg_Rc:0.3f} \\t F1:{avg_F1:0.3f}'\n",
    "experiment_summary.append(expt_summary)\n",
    "print('*')\n",
    "print(120*'=')\n",
    "print(expt_summary)\n",
    "print(120*'=')\n",
    "print('*')\n",
    "\n",
    "print(f'- REINFORCE Test results written to file: {RESULTS_FILE}.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1e69e254-d356-41e7-9f29-b5b8c42721a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** REINFORCE model performance satisfactory. Saving model to models/0_0_PHM_C01_SS_NoNBD__RF_Model_25-Dec-2024_13-32_1.00_0.67_0.91.mdl ***\n",
      "\n",
      "* Model saved to models/0_0_PHM_C01_SS_NoNBD__RF_Model_25-Dec-2024_13-32_1.00_0.67_0.91.mdl.\n",
      "* Test Report: Algorithm level consolidated metrics will be written to: results//0_0_PHM_C01_SS_NoNBD__metrics.csv.\n",
      "- Experiment related meta info written.\n",
      "- Algorithm level consolidated metrics reported to file.\n",
      "- results//TEST_CONSOLIDATED_METRICS.csv file updated.\n",
      "          Precision      Recall        F_Beta_0_5        F_Beta_0_75         \\\n",
      "               mean  std   mean    std       mean    std        mean    std   \n",
      "Algorithm                                                                     \n",
      "REINFORCE       1.0  0.0  0.665  0.053      0.907  0.021       0.712  0.049   \n",
      "\n",
      "          F_1_Score       Normal_error Replace_error Overall_error  \n",
      "               mean   std         mean          mean          mean  \n",
      "Algorithm                                                           \n",
      "REINFORCE     0.798  0.04          0.0         0.335         0.168  \n",
      "- Updating summary performance metrics.\n",
      " - Updating Sensitivity Anlysis metrics...\n",
      "- End Experiment 0-0\n"
     ]
    }
   ],
   "source": [
    "### Create a consolidated algorithm wise metrics summary\n",
    "## Add model training hyper parameters and save model, if metrics > 0.65\n",
    "if avg_Pr > MIN_MODEL_PERFORMANCE and avg_Rc > MIN_MODEL_PERFORMANCE and avg_F1 > MIN_MODEL_PERFORMANCE:\n",
    "    model_file_RF = f'models/{VERSION}_RF_Model_{dt_m}_{avg_Pr:.2f}_{avg_Rc:.2f}_{avg_F1:.2f}.mdl'\n",
    "    print(f'\\n*** REINFORCE model performance satisfactory. Saving model to {model_file_RF} ***\\n')\n",
    "\n",
    "    agent_RF.model_parameters = {'R1':R1, 'R2':R2, 'R3':R3, 'WEAR_THRESHOLD':WEAR_THRESHOLD, 'THRESHOLD_FACTOR':THRESHOLD_FACTOR, \\\n",
    "                                 'ADD_NOISE':ADD_NOISE, 'BREAKDOWN_CHANCE':BREAKDOWN_CHANCE, 'EPISODES':EPISODES, \\\n",
    "                                 'MILLING_OPERATIONS_MAX':MILLING_OPERATIONS_MAX, 'Learning_Rate': alpha}\n",
    "    save_model(agent_RF, model_file_RF)\n",
    "    df_expts.loc[n_expt, 'model_file'] = model_file_RF\n",
    "    \n",
    "    print(f'* Test Report: Algorithm level consolidated metrics will be written to: {METRICS_FILE}.')\n",
    "\n",
    "    header_columns = [VERSION]\n",
    "    write_test_results(header_columns, METRICS_FILE)\n",
    "    header_columns = ['Date', 'Time', 'Environment', 'Noise', 'Breakdown_chance', 'Train_data', 'env.R1', 'env.R2', 'env.R3', 'Wear threshold', 'Look-ahead Factor', 'Episodes', 'Terminate on', 'Test_info', 'Test_cases', 'Metrics_method', 'Version']\n",
    "    write_test_results(header_columns, METRICS_FILE)\n",
    "\n",
    "    dt_t = dt.strftime('%H:%M:%S')\n",
    "    noise_info = 'None' if ADD_NOISE == 0 else (1/ADD_NOISE)\n",
    "    header_info = [dt_d, dt_t, ENVIRONMENT_INFO, noise_info, BREAKDOWN_CHANCE, DATA_FILE, env.R1, env.R2, env.R3, WEAR_THRESHOLD, THRESHOLD_FACTOR, EPISODES, MILLING_OPERATIONS_MAX, TEST_INFO, TEST_CASES, METRICS_METHOD, VERSION]\n",
    "    write_test_results(header_info, METRICS_FILE)\n",
    "    write_test_results([], METRICS_FILE) # leave a blank line\n",
    "    print('- Experiment related meta info written.')\n",
    "\n",
    "    df_algo_results = pd.read_csv(RESULTS_FILE)\n",
    "    # algo_metrics = compute_metrics_simple(df_algo_results)\n",
    "    algo_metrics = compute_metrics(df_algo_results)\n",
    "\n",
    "    write_metrics_report(algo_metrics, METRICS_FILE, 4)\n",
    "    write_test_results([], METRICS_FILE) # leave a blank line\n",
    "    print('- Algorithm level consolidated metrics reported to file.')\n",
    "\n",
    "    write_test_results(header_columns, CONSOLIDATED_METRICS_FILE)\n",
    "    write_test_results(header_info, CONSOLIDATED_METRICS_FILE)\n",
    "    write_test_results([], CONSOLIDATED_METRICS_FILE) # leave a blank line\n",
    "    write_metrics_report(algo_metrics, CONSOLIDATED_METRICS_FILE, 4)\n",
    "    write_test_results([120*'-'], CONSOLIDATED_METRICS_FILE) # leave a blank line\n",
    "    print(f'- {CONSOLIDATED_METRICS_FILE} file updated.')\n",
    "    print(algo_metrics.round(3))\n",
    "\n",
    "    # $$$ Update\n",
    "    print(f'- Updating summary performance metrics.')\n",
    "    sensitivity_anlysis_metrics(df_expts, n_expt, algo_metrics)\n",
    "\n",
    "    print(f'- End Experiment {n_tr_round}-{n_expt}')\n",
    "else:\n",
    "    clean_up_files(RESULTS_FOLDER, VERSION, dt_d, dt_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2585939b-0a2b-4aeb-92e7-20880e8774b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
