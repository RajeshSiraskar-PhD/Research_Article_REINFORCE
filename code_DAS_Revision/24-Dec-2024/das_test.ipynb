{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f14dbfd3-f967-439c-b158-7e19b9bafbc4",
   "metadata": {},
   "source": [
    "## DAS Article Python Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ee21e98-27ae-48ee-8332-e45cbcbbb42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ====== REINFORCE for Predictive Maintenance ======\n",
      " **************** TEST DAS 1****************\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "* Experiments file: TestRun.csv -- Rounds 2\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "START_ROUND = 0\n",
    "TRAINING_ROUNDS = 2\n",
    "EXPTS_SETTINGS = 'TestRun.csv'\n",
    "MIN_MODEL_PERFORMANCE = -1.0 # Set to 0.70, to auto save models with metrics > 0.7\n",
    "SB3_EPISODES = 100\n",
    "\n",
    "print ('\\n ====== REINFORCE for Predictive Maintenance ======')\n",
    "print (' **************** TEST DAS 1****************')\n",
    "print (120*'-')\n",
    "print (f'* Experiments file: {EXPTS_SETTINGS} -- Rounds {TRAINING_ROUNDS}')\n",
    "print (120*'=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23994053-ed36-4ad9-b558-99c78d41bbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Loading packages...\n"
     ]
    }
   ],
   "source": [
    "print ('- Loading packages...')\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3 import A2C, PPO, DQN\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "from milling_tool_environment import MillingTool_SS_NT, MillingTool_MS_V3\n",
    "from utilities import compute_metrics, compute_metrics_simple, write_metrics_report, store_results, plot_learning_curve, single_axes_plot, lnoise\n",
    "from utilities import two_axes_plot, two_variable_plot, plot_error_bounds, test_script, write_test_results, downsample, save_model, load_model, clean_up_files\n",
    "from utilities import add_performance_columns, summary_performance_metrics\n",
    "from reinforce_classes import PolicyNetwork, Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb8cee1a-4368-4a74-88d4-129d87f245eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_summary = []\n",
    "n_tr_round = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21fc0c57-a7a6-41fd-b4cd-a0a09dfab340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Loading Experiments...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Auto experiment file structure\n",
    "print ('- Loading Experiments...')\n",
    "df_expts = pd.read_csv(EXPTS_SETTINGS)\n",
    "\n",
    "# Add round number column to Experiments files\n",
    "df_expts['Round'] = n_tr_round\n",
    "# Initialize columns for recording model performance\n",
    "df_expts = add_performance_columns(df_expts)\n",
    "df_expts['model_file'] = 'Not satisfactory'\n",
    "\n",
    "# Initialize record training time\n",
    "df_expts['RF_time'] = 0.0\n",
    "df_expts['A2C_time'] = 0.0\n",
    "df_expts['DQN_time'] = 0.0\n",
    "df_expts['PPO_time'] = 0.0\n",
    "\n",
    "n_expts = len(df_expts.index)\n",
    "n_expt = 0\n",
    "n_expt, n_expts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c50949f-0c79-465d-b352-b03155e13412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "************************************************************************************************************************\n",
      "************************************************************************************************************************\n",
      " * Round-0 Experiment 0 | Dasic_NoNBD\n",
      "========================================================================================================================\n",
      "\n",
      "- Columns added to results file:  results//0_0_Dasic_NoNBD__test_results_24-Dec-2024_20-15.csv\n"
     ]
    }
   ],
   "source": [
    "dt = datetime.now()\n",
    "dt_d = dt.strftime('%d-%b-%Y')\n",
    "dt_t = dt.strftime('%H-%M')\n",
    "dt_m = f'{dt_d}_{dt_t}'\n",
    "# dt_m = dt.strftime('%d-%H%M')\n",
    "\n",
    "# Load experiment parameters\n",
    "ENVIRONMENT_CLASS = df_expts['environment'][n_expt]\n",
    "ENVIRONMENT_INFO = df_expts['environment_info'][n_expt]\n",
    "ENVIRONMENT_INFO = f'{ENVIRONMENT_INFO}-{ENVIRONMENT_CLASS}'\n",
    "DATA_FILE = df_expts['data_file'][n_expt]\n",
    "R1 = df_expts['R1'][n_expt]\n",
    "R2 = df_expts['R2'][n_expt]\n",
    "R3 = df_expts['R3'][n_expt]\n",
    "WEAR_THRESHOLD = df_expts['wear_threshold'][n_expt]\n",
    "THRESHOLD_FACTOR = df_expts['threshold_factor'][n_expt]\n",
    "ADD_NOISE = df_expts['add_noise'][n_expt]\n",
    "BREAKDOWN_CHANCE = df_expts['breakdown_chance'][n_expt]\n",
    "EPISODES = df_expts['episodes'][n_expt]\n",
    "MILLING_OPERATIONS_MAX = df_expts['milling_operations_max'][n_expt]\n",
    "ver_prefix = df_expts['version_prefix'][n_expt]\n",
    "TEST_INFO = df_expts['test_info'][n_expt]\n",
    "TEST_CASES = df_expts['test_cases'][n_expt]\n",
    "TEST_ROUNDS = df_expts['test_rounds'][n_expt]\n",
    "RESULTS_FOLDER = df_expts['results_folder'][n_expt]\n",
    "\n",
    "TEST_FILE = df_expts['test_file'][n_expt]\n",
    "TRAIN_SR = df_expts['train_sample_rate'][n_expt]\n",
    "TEST_SR = df_expts['test_sample_rate'][n_expt]\n",
    "\n",
    "## Read data\n",
    "df = pd.read_csv(DATA_FILE)\n",
    "n_records = len(df.index)\n",
    "l_noise = lnoise(ADD_NOISE, BREAKDOWN_CHANCE)\n",
    "#VERSION = f'{ver_prefix}_{l_noise}_{WEAR_THRESHOLD}_{THRESHOLD_FACTOR}_{R3}_{EPISODES}_{MILLING_OPERATIONS_MAX}_'\n",
    "VERSION = f'{n_tr_round}_{n_expt}_{ver_prefix}_{l_noise}_'\n",
    "print('\\n\\n')\n",
    "print(120*'*')\n",
    "print(120*'*')\n",
    "print(f' * Round-{n_tr_round} Experiment {n_expt} | {ver_prefix}_{l_noise}')\n",
    "print(120*'=')\n",
    "\n",
    "METRICS_METHOD = 'binary' # average method = {‘micro’, ‘macro’, ‘samples’, ‘weighted’, ‘binary’}\n",
    "WEAR_THRESHOLD_NORMALIZED = 0.0 # normalized to the max wear threshold\n",
    "\n",
    "# Policy network learning parameters\n",
    "gamma = 0.99\n",
    "alpha = 0.01\n",
    "\n",
    "CONSOLIDATED_METRICS_FILE = f'{RESULTS_FOLDER}/TEST_CONSOLIDATED_METRICS.csv'\n",
    "RESULTS_FILE = f'{RESULTS_FOLDER}/{VERSION}_test_results_{dt_m}.csv'\n",
    "METRICS_FILE = f'{RESULTS_FOLDER}/{VERSION}_metrics.csv'\n",
    "END_ROUND = START_ROUND + TRAINING_ROUNDS\n",
    "EXPTS_REPORT = f'{RESULTS_FOLDER}/Experiment_Results_{START_ROUND}_{END_ROUND}_{n_tr_round}.csv'\n",
    "\n",
    "logdir = './tensorboard/'\n",
    "\n",
    "print('\\n- Columns added to results file: ', RESULTS_FILE)\n",
    "results = ['Date', 'Time', 'Round', 'Environment', 'Training_data', 'Wear_Threshold', 'Test_data', 'Algorithm', 'Episodes', 'Normal_cases', 'Normal_error',\n",
    "           'Replace_cases', 'Replace_error', 'Overall_error',\n",
    "           'Precision', 'Recall', 'F_Beta_0_5', 'F_Beta_0_75', 'F_1_Score']\n",
    "write_test_results(results, RESULTS_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79689c7d-e60a-4108-8c8b-89a550bb87a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Tool wear data imported (121 records).\n"
     ]
    }
   ],
   "source": [
    "# 1. Add noise\n",
    "if ADD_NOISE:\n",
    "    df['tool_wear'] += np.random.normal(0, 1, n_records)/ADD_NOISE\n",
    "\n",
    "# 2. Add ACTION code\n",
    "df['ACTION_CODE'] = np.where(df['tool_wear'] < WEAR_THRESHOLD, 0.0, 1.0)\n",
    "\n",
    "# 3. Normalize\n",
    "WEAR_MIN = df['tool_wear'].min()\n",
    "WEAR_MAX = df['tool_wear'].max()\n",
    "WEAR_THRESHOLD_ORG_NORMALIZED = (WEAR_THRESHOLD-WEAR_MIN)/(WEAR_MAX-WEAR_MIN)\n",
    "WEAR_THRESHOLD_NORMALIZED = THRESHOLD_FACTOR*(WEAR_THRESHOLD-WEAR_MIN)/(WEAR_MAX-WEAR_MIN)\n",
    "df_normalized = (df-df.min())/(df.max()-df.min())\n",
    "df_normalized['ACTION_CODE'] = df['ACTION_CODE']\n",
    "print(f'- Tool wear data imported ({len(df.index)} records).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "622846a6-9278-4074-b7f2-29f7bc831d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Down-sampling. Input data records: 121. Sampling rate: 1. Expected rows 121. Down-sampled to 121 rows.\n",
      "- Down-sampling. Input data records: 121. Sampling rate: 2. Expected rows 60. Down-sampled to 61 rows.\n",
      "- Tool wear data split into train (121 records) and test (61 records).\n"
     ]
    }
   ],
   "source": [
    "# 4. Test file -or- create test file\n",
    "if TRAIN_SR:\n",
    "    # 4. Split into train and test\n",
    "    df_train = downsample(df_normalized, TRAIN_SR)\n",
    "    df_train.to_csv('TempTrain.csv')\n",
    "    df_train = pd.read_csv('TempTrain.csv')\n",
    "\n",
    "    df_test = downsample(df_normalized, TEST_SR)\n",
    "    df_test.to_csv('TempTest.csv')\n",
    "    df_test = pd.read_csv('TempTest.csv')\n",
    "    print(f'- Tool wear data split into train ({len(df_train.index)} records) and test ({len(df_test.index)} records).')\n",
    "else:\n",
    "    # 4. Split into train and test\n",
    "    df_train = df_normalized\n",
    "    df_test = pd.read_csv(TEST_FILE)\n",
    "    print(f'* Separate test data provided: {TEST_FILE} - ({len(df_test.index)} records).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "910212c4-95e0-48ce-93f6-2d3b5adee9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_records = len(df_train.index)\n",
    "x = [n for n in range(n_records)]\n",
    "y1 = df_train['tool_wear']\n",
    "y2 = df_train['ACTION_CODE']\n",
    "wear_plot = f'{RESULTS_FOLDER}/{VERSION}_wear_plot.png'\n",
    "title=f'Tool Wear (mm) data\\n{VERSION}'\n",
    "two_axes_plot(x, y1, y2, title=title, x_label='Time', y1_label='Tool Wear (mm)', y2_label='Action code (1=Replace)', xticks=20, file=wear_plot, threshold_org = WEAR_THRESHOLD_ORG_NORMALIZED, threshold=WEAR_THRESHOLD_NORMALIZED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1606c87d-f512-437e-b635-1e8f4f54f5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAS RUN\n",
      "\n",
      "\n",
      "** -- Single-variate env. wear_threshold: 0.5632 R1: 1.0, R2: -1, R3: -40. Noise: 0. Break-down chance: 0 -- **\n",
      "DAS RUN\n",
      "\n",
      "\n",
      "** -- Single-variate env. wear_threshold: 0.5929 R1: 1.0, R2: -1, R3: -40. Noise: 0. Break-down chance: 0 -- **\n"
     ]
    }
   ],
   "source": [
    "if ENVIRONMENT_CLASS == 'SS':\n",
    "    env = MillingTool_SS_NT(df_train, WEAR_THRESHOLD_NORMALIZED, MILLING_OPERATIONS_MAX, ADD_NOISE, BREAKDOWN_CHANCE, R1, R2, R3)\n",
    "    env_test = MillingTool_SS_NT(df_test, WEAR_THRESHOLD_ORG_NORMALIZED, MILLING_OPERATIONS_MAX, ADD_NOISE, BREAKDOWN_CHANCE, R1, R2, R3)\n",
    "elif ENVIRONMENT_CLASS == 'MS':\n",
    "    env = MillingTool_MS_V3(df_train, WEAR_THRESHOLD_NORMALIZED, MILLING_OPERATIONS_MAX, ADD_NOISE, BREAKDOWN_CHANCE, R1, R2, R3)\n",
    "    env_test = MillingTool_MS_V3(df_test, WEAR_THRESHOLD_ORG_NORMALIZED, MILLING_OPERATIONS_MAX, ADD_NOISE, BREAKDOWN_CHANCE, R1, R2, R3)\n",
    "else:\n",
    "    print(' ERROR - initatizing environment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "447931e7-33fb-47ae-8c0e-0a92413a9bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----NO MONITOR----\n",
      "\n",
      "* Train REINFORCE model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Enable tensorboard rewards and loss plots\n",
    "# env = Monitor(env, logdir, allow_early_resets=True)\n",
    "print('----NO MONITOR----')\n",
    "\n",
    "# ## REINFORCE RL Algorithm\n",
    "### Main loop\n",
    "print('\\n* Train REINFORCE model...')\n",
    "rewards_history = []\n",
    "loss_history = []\n",
    "training_stats = []\n",
    "\n",
    "input_dim = env.observation_space.shape[0]\n",
    "output_dim = env.action_space.n\n",
    "\n",
    "agent_RF = Agent(input_dim, output_dim, alpha, gamma)\n",
    "\n",
    "EPISODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4339df8c-1586-417c-94b2-6361195628b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0000] Loss:  -5.64e-03 | Reward:  -1.17e-03 | Ep.length: 0010\n"
     ]
    }
   ],
   "source": [
    "# $$$ \n",
    "truncated = False\n",
    "\n",
    "time_RF = time.time()\n",
    "for episode in range(EPISODES):\n",
    "    state = env.reset()\n",
    "\n",
    "    # Sample a trajectory\n",
    "    for t in range(MILLING_OPERATIONS_MAX): # Max. milling operations desired\n",
    "        action = agent_RF.act(state)\n",
    "        # state, reward, done, truncated, info = env.step(action)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        agent_RF.rewards.append(reward)\n",
    "        #env.render()\n",
    "        if done:\n",
    "            # print('** DONE **', info)\n",
    "            break\n",
    "\n",
    "    # Learn during this episode\n",
    "    loss = agent_RF.learn() # train per episode\n",
    "    total_reward = sum(agent_RF.rewards)\n",
    "\n",
    "    # Record statistics for this episode\n",
    "    rewards_history.append(total_reward)\n",
    "    loss_history.append(loss.item()) # Extract values from list of torch items for plotting\n",
    "\n",
    "    # On-policy - so discard all data\n",
    "    agent_RF.onpolicy_reset()\n",
    "\n",
    "    if (episode%100 == 0):\n",
    "        # print(f'[{episode:04d}] Loss: {loss:>10.2f} | Reward: {total_reward:>10.2f} | Ep.length: {env.ep_length:04d}')\n",
    "        print(f'[{episode:04d}] Loss: {loss:>10.2e} | Reward: {total_reward:>10.2e} | Ep.length: {env.ep_length:04d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6b947af-5f07-49d3-90da-6a12d1961e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# end RF training\n",
    "time_RF = time.time() - time_RF\n",
    "\n",
    "x = [i for i in range(EPISODES)]\n",
    "\n",
    "# ## Moving average for rewards\n",
    "ma_window_size = 10\n",
    "# # Convert error array to pandas series\n",
    "rewards = pd.Series(rewards_history)\n",
    "windows = rewards.rolling(ma_window_size)\n",
    "moving_avg = windows.mean()\n",
    "moving_avg_lst = moving_avg.tolist()\n",
    "y1 = rewards\n",
    "y2 = moving_avg_lst\n",
    "\n",
    "filename = f'{RESULTS_FOLDER}/{VERSION}_Avg_episode_rewards.png'\n",
    "two_variable_plot(x, y1, y2, 'Avg. rewards per episode', VERSION, 'Episode', 'Avg. Rewards', 'Moving Avg.', 50, filename)\n",
    "\n",
    "# plot_error_bounds(x, y1)\n",
    "\n",
    "filename = f'{RESULTS_FOLDER}/{VERSION}_Episode_Length.png'\n",
    "single_axes_plot(x, env.ep_length_history, 'Episode length', VERSION, 'Episode', 'No of milling operations', 50, 0.0, filename)\n",
    "\n",
    "filename = f'{RESULTS_FOLDER}/{VERSION}_Tool_Replacements.png'\n",
    "single_axes_plot(x, env.ep_tool_replaced_history, 'Tool replacements per episode', VERSION, 'Episode', 'Replacements', 50, 0.0, filename)\n",
    "\n",
    "# ### Generate a balanced test set\n",
    "idx_replace_cases = df_test.index[df_test['ACTION_CODE'] >= 1.0]\n",
    "idx_normal_cases = df_test.index[df_test['ACTION_CODE'] < 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a41b9519-b4de-462f-ad14-2f9878204a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- Test REINFORCE model...\n",
      "***** SB-3 TEST CASES: 0 -- [30, 12, 40, 48]\n",
      "***** SB-3 TEST CASES: 1 -- [12, 30, 48, 40]\n",
      "***** SB-3 TEST CASES: 2 -- [30, 12, 40, 48]\n",
      "***** SB-3 TEST CASES: 3 -- [30, 12, 40, 48]\n",
      "*\n",
      "========================================================================================================================\n",
      "[0:0] >> Simulated NBD-SS - NoNBD Pr: 0.542 \t Rc: 0.625 \t F1:0.554\n",
      "========================================================================================================================\n",
      "*\n",
      "- REINFORCE Test results written to file: results//0_0_Dasic_NoNBD__test_results_24-Dec-2024_20-15.csv.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Process results\n",
    "# eps = [i for i in range(EPISODES)]\n",
    "# store_results(RF_TRAINING_FILE, training_round, eps, rewards_history, env.ep_tool_replaced_history)\n",
    "print('\\n- Test REINFORCE model...')\n",
    "# print(80*'-')\n",
    "# print(f'Algorithm\\tNormal\\terr.%\\tReplace\\terr.%\\tOverall err.%')\n",
    "# print(80*'-')\n",
    "avg_Pr = avg_Rc = avg_F1 = 0.0\n",
    "\n",
    "test_case_collection = []\n",
    "for test_round in range(TEST_ROUNDS):\n",
    "    # Create test cases\n",
    "    idx_replace_cases = np.random.choice(idx_replace_cases, int(TEST_CASES/2), replace=False)\n",
    "    idx_normal_cases = np.random.choice(idx_normal_cases, int(TEST_CASES/2), replace=False)\n",
    "    test_cases = [*idx_normal_cases, *idx_replace_cases]\n",
    "    test_case_collection.append(test_cases)\n",
    "    print(f'***** SB-3 TEST CASES: {test_round} -- {test_case_collection[test_round]}')\n",
    "\n",
    "    results = test_script(METRICS_METHOD, test_round, df_test, 'REINFORCE', EPISODES, env_test, ENVIRONMENT_INFO, agent_RF,\n",
    "                          test_cases, TEST_INFO, DATA_FILE, WEAR_THRESHOLD, RESULTS_FILE)\n",
    "    write_test_results(results, RESULTS_FILE)\n",
    "    avg_Pr += results[14]\n",
    "    avg_Rc += results[15]\n",
    "    avg_F1 += results[16]\n",
    "\n",
    "avg_Pr = avg_Pr/TEST_ROUNDS\n",
    "avg_Rc = avg_Rc/TEST_ROUNDS\n",
    "avg_F1 = avg_F1/TEST_ROUNDS\n",
    "\n",
    "# df_expts.loc[n_expt, 'Pr'] = avg_Pr\n",
    "# df_expts.loc[n_expt, 'Rc'] = avg_Rc\n",
    "# df_expts.loc[n_expt, 'F1'] = avg_F1\n",
    "\n",
    "df_expts.loc[n_expt, 'RF_time'] = time_RF\n",
    "\n",
    "expt_summary = f'[{n_tr_round}:{n_expt}] >> {ENVIRONMENT_INFO} - {l_noise} Pr: {avg_Pr:0.3f} \\t Rc: {avg_Rc:0.3f} \\t F1:{avg_F1:0.3f}'\n",
    "experiment_summary.append(expt_summary)\n",
    "print('*')\n",
    "print(120*'=')\n",
    "print(expt_summary)\n",
    "print(120*'=')\n",
    "print('*')\n",
    "\n",
    "print(f'- REINFORCE Test results written to file: {RESULTS_FILE}.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33cdc69a-d2e3-48a9-bb44-f171d40b163a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** REINFORCE model performance satisfactory. Saving model to models/0_0_Dasic_NoNBD__RF_Model_24-Dec-2024_20-15_0.54_0.62_0.55.mdl ***\n",
      "\n",
      "* Model saved to models/0_0_Dasic_NoNBD__RF_Model_24-Dec-2024_20-15_0.54_0.62_0.55.mdl.\n",
      "* Train Stable-Baselines-3 A2C, DQN and PPO models...\n",
      "- Training Stable-Baselines-3 A2C algorithm...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "MillingTool_SS_NT.reset() got an unexpected keyword argument 'seed'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m- Training Stable-Baselines-3 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSB_ALGO\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m algorithm...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     23\u001b[0m time_SB \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 24\u001b[0m \u001b[43magent_SB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m time_SB \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m time_SB\n\u001b[0;32m     26\u001b[0m time_column \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSB_ALGO\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_time\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\RL\\lib\\site-packages\\stable_baselines3\\a2c\\a2c.py:201\u001b[0m, in \u001b[0;36mA2C.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfA2C,\n\u001b[0;32m    194\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    199\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    200\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfA2C:\n\u001b[1;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:287\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfOnPolicyAlgorithm,\n\u001b[0;32m    278\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    283\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    284\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfOnPolicyAlgorithm:\n\u001b[0;32m    285\u001b[0m     iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 287\u001b[0m     total_timesteps, callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_learn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    295\u001b[0m     callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\base_class.py:423\u001b[0m, in \u001b[0;36mBaseAlgorithm._setup_learn\u001b[1;34m(self, total_timesteps, callback, reset_num_timesteps, tb_log_name, progress_bar)\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reset_num_timesteps \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    422\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 423\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m    424\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_episode_starts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mnum_envs,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[0;32m    425\u001b[0m     \u001b[38;5;66;03m# Retrieve unnormalized observation for saving into the buffer\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:77\u001b[0m, in \u001b[0;36mDummyVecEnv.reset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m     76\u001b[0m     maybe_options \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptions\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options[env_idx]} \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options[env_idx] \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m---> 77\u001b[0m     obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs[env_idx]\u001b[38;5;241m.\u001b[39mreset(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_seeds[env_idx], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmaybe_options)\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_obs(env_idx, obs)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Seeds and options are only used once\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\monitor.py:83\u001b[0m, in \u001b[0;36mMonitor.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected you to pass keyword argument \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m into reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_reset_info[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mreset(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: MillingTool_SS_NT.reset() got an unexpected keyword argument 'seed'"
     ]
    }
   ],
   "source": [
    "## Add model training hyper parameters and save model, if metrics > 0.65\n",
    "if avg_Pr > MIN_MODEL_PERFORMANCE and avg_Rc > MIN_MODEL_PERFORMANCE and avg_F1 > MIN_MODEL_PERFORMANCE:\n",
    "    model_file_RF = f'models/{VERSION}_RF_Model_{dt_m}_{avg_Pr:.2f}_{avg_Rc:.2f}_{avg_F1:.2f}.mdl'\n",
    "    print(f'\\n*** REINFORCE model performance satisfactory. Saving model to {model_file_RF} ***\\n')\n",
    "\n",
    "    agent_RF.model_parameters = {'R1':R1, 'R2':R2, 'R3':R3, 'WEAR_THRESHOLD':WEAR_THRESHOLD, 'THRESHOLD_FACTOR':THRESHOLD_FACTOR, 'ADD_NOISE':ADD_NOISE, 'BREAKDOWN_CHANCE':BREAKDOWN_CHANCE, 'EPISODES':EPISODES, 'MILLING_OPERATIONS_MAX':MILLING_OPERATIONS_MAX}\n",
    "    save_model(agent_RF, model_file_RF)\n",
    "    df_expts.loc[n_expt, 'model_file'] = model_file_RF\n",
    "\n",
    "    # ## Stable-Baselines Algorithms\n",
    "    print('* Train Stable-Baselines-3 A2C, DQN and PPO models...')\n",
    "\n",
    "    total_timesteps = SB3_EPISODES\n",
    "    algos = ['A2C','DQN','PPO']\n",
    "    SB_agents = []\n",
    "\n",
    "    for SB_ALGO in algos:\n",
    "        if SB_ALGO.upper() == 'A2C': agent_SB = A2C('MlpPolicy', env)\n",
    "        if SB_ALGO.upper() == 'DQN': agent_SB = DQN('MlpPolicy', env)\n",
    "        if SB_ALGO.upper() == 'PPO': agent_SB = PPO('MlpPolicy', env)\n",
    "\n",
    "        print(f'- Training Stable-Baselines-3 {SB_ALGO} algorithm...')\n",
    "        time_SB = time.time()\n",
    "        agent_SB.learn(total_timesteps=total_timesteps)\n",
    "        time_SB = time.time() - time_SB\n",
    "        time_column = f'{SB_ALGO.upper()}_time'\n",
    "        df_expts.loc[n_expt, time_column] = time_SB\n",
    "\n",
    "        SB_agents.append(agent_SB)\n",
    "    n = 0\n",
    "    for agent_SB in SB_agents:\n",
    "        print(f'- Testing Stable-Baselines-3 {agent_SB} model...')\n",
    "        for test_round in range(TEST_ROUNDS):\n",
    "            # Extract the test cases from the 'test_case_collection' list\n",
    "            test_cases = test_case_collection[test_round]\n",
    "            print(f'***** SB-3 TEST CASES: {test_round} -- {test_cases}')\n",
    "            results = test_script(METRICS_METHOD, test_round, df_test, algos[n], EPISODES, env_test, ENVIRONMENT_INFO,\n",
    "                                  agent_SB, test_cases, TEST_INFO, DATA_FILE, WEAR_THRESHOLD, RESULTS_FILE)\n",
    "            write_test_results(results, RESULTS_FILE)\n",
    "            # end test loop\n",
    "\n",
    "        # Save SB model with results appended\n",
    "        sb_Pr = results[14]\n",
    "        sb_Rc = results[15]\n",
    "        sb_F1 = results[18]\n",
    "        model_file_SB = f'models/{VERSION}_{algos[n]}_{dt_m}_{sb_Pr:.2f}_{sb_Rc:.2f}_{sb_F1:.2f}.mdl'\n",
    "        print(f'- Save Stable-Baselines-3 model: {model_file_SB}')\n",
    "        agent_SB.save(model_file_SB)\n",
    "\n",
    "        n += 1\n",
    "        # end SB agents loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e69e254-d356-41e7-9f29-b5b8c42721a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a consolidated algorithm wise metrics summary\n",
    "\n",
    "            print(f'* Test Report: Algorithm level consolidated metrics will be written to: {METRICS_FILE}.')\n",
    "\n",
    "            header_columns = [VERSION]\n",
    "            write_test_results(header_columns, METRICS_FILE)\n",
    "            header_columns = ['Date', 'Time', 'Environment', 'Noise', 'Breakdown_chance', 'Train_data', 'env.R1', 'env.R2', 'env.R3', 'Wear threshold', 'Look-ahead Factor', 'Episodes', 'Terminate on', 'Test_info', 'Test_cases', 'Metrics_method', 'Version']\n",
    "            write_test_results(header_columns, METRICS_FILE)\n",
    "\n",
    "            dt_t = dt.strftime('%H:%M:%S')\n",
    "            noise_info = 'None' if ADD_NOISE == 0 else (1/ADD_NOISE)\n",
    "            header_info = [dt_d, dt_t, ENVIRONMENT_INFO, noise_info, BREAKDOWN_CHANCE, DATA_FILE, env.R1, env.R2, env.R3, WEAR_THRESHOLD, THRESHOLD_FACTOR, EPISODES, MILLING_OPERATIONS_MAX, TEST_INFO, TEST_CASES, METRICS_METHOD, VERSION]\n",
    "            write_test_results(header_info, METRICS_FILE)\n",
    "            write_test_results([], METRICS_FILE) # leave a blank line\n",
    "            print('- Experiment related meta info written.')\n",
    "\n",
    "            df_algo_results = pd.read_csv(RESULTS_FILE)\n",
    "            # algo_metrics = compute_metrics_simple(df_algo_results)\n",
    "            algo_metrics = compute_metrics(df_algo_results)\n",
    "\n",
    "            write_metrics_report(algo_metrics, METRICS_FILE, 4)\n",
    "            write_test_results([], METRICS_FILE) # leave a blank line\n",
    "            print('- Algorithm level consolidated metrics reported to file.')\n",
    "\n",
    "            write_test_results(header_columns, CONSOLIDATED_METRICS_FILE)\n",
    "            write_test_results(header_info, CONSOLIDATED_METRICS_FILE)\n",
    "            write_test_results([], CONSOLIDATED_METRICS_FILE) # leave a blank line\n",
    "            write_metrics_report(algo_metrics, CONSOLIDATED_METRICS_FILE, 4)\n",
    "            write_test_results([120*'-'], CONSOLIDATED_METRICS_FILE) # leave a blank line\n",
    "            print(f'- {CONSOLIDATED_METRICS_FILE} file updated.')\n",
    "            print(algo_metrics.round(3))\n",
    "\n",
    "            # $$$ Update\n",
    "            print(f'- Updating summary performance metrics.')\n",
    "            summary_performance_metrics(df_expts, n_expt, algo_metrics)\n",
    "\n",
    "            print(f'- End Experiment {n_tr_round}-{n_expt}')\n",
    "        else:\n",
    "            clean_up_files(RESULTS_FOLDER, VERSION, dt_d, dt_m)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
